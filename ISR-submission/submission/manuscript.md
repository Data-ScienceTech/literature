% Information Systems Research — Submission
% Title: Mapping Streams of Research in Information Systems via Hybrid Text–Citation Clustering
% Author: Carlos Denner dos Santos (Corresponding: carlosdenner@unb.br)
% Date: October 07, 2025

# Abstract
We develop and evaluate a scalable, reproducible pipeline to map the intellectual structure of the Information Systems field by jointly leveraging textual content (TF–IDF + LSI) and citation structure (bibliographic coupling) in a hybrid embedding, followed by multi-level clustering (L1–L3). The approach discovers coherent high-level streams, sub-topics, and fine-grained research niches, and it validates them with internal metrics and manual checks. We release code, data-processing scripts, and an interactive explorer to support transparency and reuse. This paper contributes: (1) an end-to-end, minutes-scale pipeline for large literatures; (2) a hybrid representation that improves cluster quality over text-only and network-only baselines; and (3) an empirically grounded taxonomy of IS research with actionable implications for scholars and editors.

# 1. Introduction
The rapid growth and fragmentation of IS scholarship complicate cumulative knowledge building and field overviews. We propose a method to discover, validate, and communicate **streams of research** using a hybrid text–citation representation and multi-level clustering. Our objectives are to: (i) design a transparent, reproducible pipeline; (ii) integrate textual semantics with citation structure to improve cluster quality; (iii) produce a three-level taxonomy (streams → topics → niches); (iv) validate clusters quantitatively and qualitatively; and (v) provide open tools (scripts + explorer) for ongoing maintenance of the map.

**Contributions.** Methodologically, we operationalize a computational literature mapping workflow that is fast and reproducible. Substantively, we provide an integrated, empirically derived taxonomy of IS research and show temporal dynamics across decades. Practically, we deliver an interactive explorer to support curriculum design, review studies, and research positioning.

# 2. Related Work
We build on structured literature reviews, science-of-science mapping, and topic identification in IS. Prior work often uses text-only topic models or citation-only clustering. We instead integrate both, showing that a hybrid embedding yields more coherent clusters while retaining interpretability. We also follow best practices for transparent, reproducible reviews.

# 3. Data and Methods
## 3.1 Corpus and Citation Enrichment
We construct a journal-focused corpus (top-tier IS outlets) and enrich records with reference lists to enable bibliographic coupling. Data are cleaned and deduplicated; language is restricted to English for consistency.

## 3.2 Text Features
We preprocess titles/abstracts and compute TF–IDF, then reduce via Latent Semantic Indexing (LSI).

## 3.3 Citation Network Features
We compute bibliographic coupling strengths using an inverted-index optimization to avoid O(n^2) pairwise costs.

## 3.4 Hybrid Representation
We combine normalized text and network vectors with weight w (default 0.60 text / 0.40 network), selected via validation.

## 3.5 Multi-Level Clustering and Validation
We cluster the hybrid space into L1 (streams), then recluster within clusters for L2 (topics) and L3 (niches). We evaluate with silhouette, topic coherence, and targeted manual checks. Hyperparameters are probed via sensitivity analysis (Appendix B).

## 3.6 Interactive Explorer and Reproducibility
We publish scripts, a RUNBOOK, and an explorer that allow readers to audit, browse, and export results. Environment files (requirements.txt / environment.yml) support quick setup.

# 4. Results
## 4.1 Level 1 Streams (8 Streams)
Table 1 summarizes the eight macro-streams with labels and indicative keywords (CSV generated by `tools/make_table1.py`). Each stream aggregates thematically coherent areas spanning classic and emerging lines of inquiry.

## 4.2 Level 2 Topics and 4.3 Level 3 Niches
Appendix A lists all L2 topics per stream. L3 niches capture finer granularity discoverable in the interactive explorer.

## 4.4 Temporal and Network Structure
Temporal trends indicate differential growth/decline across streams; citation-network measures (e.g., density, betweenness) reveal structural positions of streams/topics. Detailed statistics are included in the explorer and artifacts directory.

# 5. Discussion
**Key Findings.** A hybrid embedding improves cluster quality versus single-modality baselines; multi-level clustering yields an interpretable taxonomy aligned with domain intuition.  
**Implications.** For scholars, the map guides positioning and review scoping; for editors, it informs desk triage and reviewer matching.  
**Methodological Notes.** The pipeline is modular and auditable; parameter choices are documented; sensitivity results indicate robustness within practical ranges.

**Limitations.** Coverage is constrained by corpus selection and English-only processing; abstracts provide partial semantics; cluster labels are post-hoc and involve judgment.  
**Future Work.** Extend venues/years, incorporate full-text where available, and add dynamic updates for continuous field monitoring.

# 6. Conclusion
We present an auditable and efficient method to surface streams of research in IS via hybrid text–citation clustering and multi-level structure. Open code, data-processing recipes, and an explorer support replication, critique, and extension by the community.

# Author Contributions (CRediT)
Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing—original draft, Writing—review & editing: **Carlos Denner dos Santos**.

# Acknowledgments
We thank colleagues and seminar participants for constructive feedback. Any errors are our own.

# Funding
No external funding was received for this work.

# Ethics/IRB
Not applicable.

# Conflicts of Interest
The author declares no conflicts of interest.

# Data and Code Availability
All scripts and instructions to reproduce tables/figures are provided in the repository and in `submission/RUNBOOK.md`. Appendix C lists core components.

# References
(References are auto-generated from in-text citations via `tools/build_bib.py`.)
