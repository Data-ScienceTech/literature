{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - BibTeX Parsing and Data Preparation\n",
    "\n",
    "This notebook parses the BibTeX file and prepares the dataset for analysis.\n",
    "\n",
    "**Input:** `refs_2016_2025_AMR_MISQ_ORSC_ISR.bib`  \n",
    "**Output:** Cleaned DataFrame with paper metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from parse_bib import load_bib, get_corpus_stats\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Parse BibTeX File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BibTeX file\n",
    "bib_path = \"../refs_2016_2025_AMR_MISQ_ORSC_ISR.bib\"\n",
    "\n",
    "print(f\"Loading BibTeX file: {bib_path}\")\n",
    "df = load_bib(bib_path)\n",
    "\n",
    "print(f\"\\nLoaded {len(df)} papers\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "stats = get_corpus_stats(df)\n",
    "\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "for key, value in stats.items():\n",
    "    if key not in ['journals', 'papers_per_year']:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample records\n",
    "print(\"=== Sample Records ===\")\n",
    "display(df[['title', 'authors', 'year', 'journal', 'doi']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data analysis\n",
    "missing_data = df.isnull().sum()\n",
    "missing_pct = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing %', ascending=False)\n",
    "\n",
    "print(\"=== Missing Data Analysis ===\")\n",
    "display(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "if 'text' in df.columns:\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    \n",
    "    print(\"=== Text Length Statistics ===\")\n",
    "    print(df['text_length'].describe())\n",
    "    \n",
    "    # Plot text length distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df['text_length'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Text Length (Title + Abstract)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per year\n",
    "if 'year' in df.columns:\n",
    "    year_counts = df['year'].value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    year_counts.plot(kind='bar', alpha=0.8)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.title('Publications per Year')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Peak year: {year_counts.idxmax()} ({year_counts.max()} papers)\")\n",
    "    print(f\"Total papers: {year_counts.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journal distribution\n",
    "if 'journal' in df.columns:\n",
    "    journal_counts = df['journal'].value_counts()\n",
    "    \n",
    "    print(\"=== Journal Distribution ===\")\n",
    "    display(journal_counts.head(10))\n",
    "    \n",
    "    # Plot journal distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    journal_counts.head(10).plot(kind='bar', alpha=0.8)\n",
    "    plt.xlabel('Journal')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.title('Top 10 Journals by Paper Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journal-year heatmap\n",
    "if 'journal' in df.columns and 'year' in df.columns:\n",
    "    # Focus on top journals\n",
    "    top_journals = journal_counts.head(4).index\n",
    "    journal_year = df[df['journal'].isin(top_journals)].groupby(['journal', 'year']).size().unstack(fill_value=0)\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.heatmap(journal_year, annot=True, fmt='d', cmap='YlOrRd', cbar_kws={'label': 'Number of Papers'})\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Journal')\n",
    "    plt.title('Publications by Journal and Year (Top 4 Journals)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References analysis\n",
    "if 'references_count' in df.columns:\n",
    "    ref_stats = df['references_count'].describe()\n",
    "    \n",
    "    print(\"=== Reference Count Statistics ===\")\n",
    "    print(ref_stats)\n",
    "    \n",
    "    # Plot reference count distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df['references_count'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Number of References')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Reference Counts')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cited DOI analysis\n",
    "if 'cited_doi' in df.columns:\n",
    "    papers_with_citations = df['cited_doi'].notna().sum()\n",
    "    citation_coverage = papers_with_citations / len(df) * 100\n",
    "    \n",
    "    print(f\"Papers with cited DOIs: {papers_with_citations} ({citation_coverage:.1f}%)\")\n",
    "    \n",
    "    # Sample cited DOIs\n",
    "    sample_citations = df[df['cited_doi'].notna()]['cited_doi'].iloc[0]\n",
    "    print(f\"\\nSample cited DOIs (first paper):\")\n",
    "    print(sample_citations[:200] + \"...\" if len(sample_citations) > 200 else sample_citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and prepare data for embedding/clustering\n",
    "# Keep papers with text (title + abstract or just title)\n",
    "analysis_df = df[df['text'].notna() & (df['text'].str.len() > 10)].copy()\n",
    "\n",
    "print(f\"Papers suitable for analysis: {len(analysis_df)} / {len(df)} ({len(analysis_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Add text statistics\n",
    "analysis_df['has_abstract'] = analysis_df['abstract'].notna()\n",
    "analysis_df['word_count'] = analysis_df['text'].str.split().str.len()\n",
    "\n",
    "print(f\"Papers with abstracts: {analysis_df['has_abstract'].sum()} ({analysis_df['has_abstract'].mean()*100:.1f}%)\")\n",
    "print(f\"Mean word count: {analysis_df['word_count'].mean():.1f}\")\n",
    "print(f\"Median word count: {analysis_df['word_count'].median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = Path('../data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save full dataset\n",
    "df.to_csv(data_dir / 'parsed_papers_full.csv', index=False)\n",
    "print(f\"Saved full dataset: {len(df)} papers\")\n",
    "\n",
    "# Save analysis-ready dataset\n",
    "analysis_df.to_csv(data_dir / 'parsed_papers_analysis.csv', index=False)\n",
    "print(f\"Saved analysis dataset: {len(analysis_df)} papers\")\n",
    "\n",
    "# Save statistics\n",
    "import json\n",
    "with open(data_dir / 'corpus_stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2, default=str)\n",
    "print(\"Saved corpus statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "1. **Parsed the BibTeX file** with robust handling of encoding and LaTeX formatting\n",
    "2. **Analyzed data quality** including missing values and text lengths\n",
    "3. **Explored temporal patterns** showing publication trends over time\n",
    "4. **Examined journal distribution** across the four target journals\n",
    "5. **Analyzed citation data** including reference counts and cited DOIs\n",
    "6. **Prepared clean datasets** for downstream analysis\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to `02_embed_cluster.ipynb` for embedding generation and clustering\n",
    "- The analysis-ready dataset contains papers suitable for text analysis\n",
    "- Citation data is available for network analysis in later notebooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
