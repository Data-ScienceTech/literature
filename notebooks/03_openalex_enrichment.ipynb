{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - OpenAlex Citation Enrichment\n",
    "\n",
    "This notebook enriches the dataset with citation data from OpenAlex API.\n",
    "\n",
    "**Input:** Clustered papers with DOIs  \n",
    "**Output:** Papers enriched with citation networks and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from openalex import OpenAlexClient, enrich_dataframe, save_cache_summary\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Clustered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clustered dataset\n",
    "data_dir = Path('../data')\n",
    "df = pd.read_csv(data_dir / 'papers_clustered.csv')\n",
    "\n",
    "print(f\"Loaded {len(df)} papers\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Check DOI availability\n",
    "doi_stats = {\n",
    "    'total_papers': len(df),\n",
    "    'papers_with_doi': df['doi'].notna().sum(),\n",
    "    'unique_dois': df['doi'].nunique(),\n",
    "    'doi_coverage': df['doi'].notna().mean() * 100\n",
    "}\n",
    "\n",
    "print(f\"\\n=== DOI Statistics ===\")\n",
    "for key, value in doi_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.1f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize OpenAlex Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAlex client with caching\n",
    "cache_dir = data_dir / 'openalex_cache'\n",
    "client = OpenAlexClient(\n",
    "    cache_dir=str(cache_dir),\n",
    "    rate_limit=0.1,  # 100ms between requests (be nice to the API)\n",
    "    timeout=30,\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "print(f\"OpenAlex client initialized\")\n",
    "print(f\"Cache directory: {cache_dir}\")\n",
    "\n",
    "# Check existing cache\n",
    "if cache_dir.exists():\n",
    "    cache_summary = save_cache_summary(str(cache_dir))\n",
    "    print(f\"\\nExisting cache: {cache_summary['successful']} successful, {cache_summary['errors']} errors\")\n",
    "else:\n",
    "    print(\"No existing cache found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a few DOIs first\n",
    "test_dois = df['doi'].dropna().head(3).tolist()\n",
    "print(f\"Testing OpenAlex API with {len(test_dois)} DOIs...\")\n",
    "\n",
    "for i, doi in enumerate(test_dois):\n",
    "    print(f\"\\nTesting DOI {i+1}: {doi}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = client.fetch_work(doi)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    if result and 'error' not in result:\n",
    "        print(f\"  ✓ Success ({elapsed:.2f}s)\")\n",
    "        print(f\"  Title: {result.get('title', 'N/A')[:60]}...\")\n",
    "        print(f\"  Citations: {result.get('cited_by_count', 0)}\")\n",
    "        print(f\"  References: {len(result.get('referenced_works', []))}\")\n",
    "    else:\n",
    "        print(f\"  ✗ Failed ({elapsed:.2f}s)\")\n",
    "        if result:\n",
    "            print(f\"  Error: {result.get('error', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich Dataset with OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if enrichment already exists\n",
    "enriched_path = data_dir / 'papers_enriched.csv'\n",
    "\n",
    "if enriched_path.exists():\n",
    "    print(\"Loading existing enriched dataset...\")\n",
    "    enriched_df = pd.read_csv(enriched_path)\n",
    "    print(f\"Loaded {len(enriched_df)} enriched papers\")\n",
    "else:\n",
    "    print(\"Starting OpenAlex enrichment...\")\n",
    "    print(\"This will take some time depending on the number of DOIs and API rate limits.\")\n",
    "    print(f\"Estimated time: {df['doi'].notna().sum() * 0.1 / 60:.1f} minutes\")\n",
    "    \n",
    "    # Enrich the dataframe\n",
    "    enriched_df = enrich_dataframe(\n",
    "        df,\n",
    "        doi_column='doi',\n",
    "        client=client,\n",
    "        max_workers=3  # Conservative to avoid overwhelming the API\n",
    "    )\n",
    "    \n",
    "    # Save enriched dataset\n",
    "    enriched_df.to_csv(enriched_path, index=False)\n",
    "    print(f\"Saved enriched dataset to {enriched_path}\")\n",
    "\n",
    "print(f\"\\nEnriched dataset shape: {enriched_df.shape}\")\n",
    "print(f\"New columns: {set(enriched_df.columns) - set(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Enrichment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrichment statistics\n",
    "enrichment_stats = {\n",
    "    'papers_with_openalex_id': enriched_df['openalex_id'].notna().sum(),\n",
    "    'papers_with_citations': enriched_df['cited_by_count'].notna().sum(),\n",
    "    'papers_with_references': enriched_df['referenced_works'].notna().sum(),\n",
    "    'papers_with_concepts': enriched_df['concepts'].notna().sum() if 'concepts' in enriched_df.columns else 0,\n",
    "    'enrichment_rate': enriched_df['openalex_id'].notna().mean() * 100\n",
    "}\n",
    "\n",
    "print(\"=== Enrichment Statistics ===\")\n",
    "for key, value in enrichment_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.1f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Citation statistics\n",
    "if 'cited_by_count' in enriched_df.columns:\n",
    "    citation_stats = enriched_df['cited_by_count'].describe()\n",
    "    print(f\"\\n=== Citation Statistics ===\")\n",
    "    print(citation_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize citation distribution\n",
    "if 'cited_by_count' in enriched_df.columns:\n",
    "    citations = enriched_df['cited_by_count'].dropna()\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(citations, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Citation Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Citation Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log scale\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(citations[citations > 0], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Citation Count (log scale)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Citation Distribution (Log Scale)')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot by cluster\n",
    "    plt.subplot(1, 3, 3)\n",
    "    top_clusters = enriched_df['cluster'].value_counts().head(8).index\n",
    "    cluster_citations = [enriched_df[enriched_df['cluster'] == c]['cited_by_count'].dropna() \n",
    "                        for c in top_clusters]\n",
    "    plt.boxplot(cluster_citations, labels=[f'C{c}' for c in top_clusters])\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Citation Count')\n",
    "    plt.title('Citations by Cluster')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze reference networks\n",
    "if 'referenced_works' in enriched_df.columns:\n",
    "    # Parse referenced works\n",
    "    all_references = []\n",
    "    papers_with_refs = enriched_df[enriched_df['referenced_works'].notna()]\n",
    "    \n",
    "    print(f\"Analyzing references from {len(papers_with_refs)} papers...\")\n",
    "    \n",
    "    for _, row in papers_with_refs.iterrows():\n",
    "        refs = row['referenced_works']\n",
    "        if isinstance(refs, str):\n",
    "            try:\n",
    "                # Try to parse as list (might be string representation)\n",
    "                import ast\n",
    "                refs = ast.literal_eval(refs)\n",
    "            except:\n",
    "                refs = []\n",
    "        \n",
    "        if isinstance(refs, list):\n",
    "            all_references.extend(refs)\n",
    "    \n",
    "    print(f\"Total references: {len(all_references)}\")\n",
    "    print(f\"Unique references: {len(set(all_references))}\")\n",
    "    \n",
    "    # Find internal citations (references within our dataset)\n",
    "    our_openalex_ids = set(enriched_df['openalex_id'].dropna())\n",
    "    internal_refs = [ref for ref in all_references if ref in our_openalex_ids]\n",
    "    \n",
    "    print(f\"Internal citations: {len(internal_refs)} ({len(internal_refs)/len(all_references)*100:.1f}%)\")\n",
    "    \n",
    "    # Reference count distribution\n",
    "    ref_counts = papers_with_refs['referenced_works'].apply(\n",
    "        lambda x: len(x) if isinstance(x, list) else (len(ast.literal_eval(x)) if isinstance(x, str) else 0)\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(ref_counts, bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Number of References')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Reference Counts per Paper')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Mean references per paper: {ref_counts.mean():.1f}\")\n",
    "    print(f\"Median references per paper: {ref_counts.median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Cited Papers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify most cited papers\n",
    "if 'cited_by_count' in enriched_df.columns:\n",
    "    most_cited = enriched_df.nlargest(10, 'cited_by_count')\n",
    "    \n",
    "    print(\"=== Top 10 Most Cited Papers ===\")\n",
    "    for i, (_, row) in enumerate(most_cited.iterrows(), 1):\n",
    "        print(f\"{i}. {row.get('title', 'N/A')[:80]}...\")\n",
    "        print(f\"   Authors: {row.get('authors', 'N/A')[:60]}...\")\n",
    "        print(f\"   Year: {row.get('year', 'N/A')}, Journal: {row.get('journal', 'N/A')}\")\n",
    "        print(f\"   Citations: {row.get('cited_by_count', 0)}, Cluster: {row.get('cluster', 'N/A')}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Citation Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze citation patterns over time\n",
    "if 'cited_by_count' in enriched_df.columns and 'year' in enriched_df.columns:\n",
    "    # Citations by year\n",
    "    year_citations = enriched_df.groupby('year')['cited_by_count'].agg(['mean', 'median', 'sum', 'count'])\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Mean citations per year\n",
    "    plt.subplot(2, 2, 1)\n",
    "    year_citations['mean'].plot(kind='line', marker='o')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Mean Citations')\n",
    "    plt.title('Mean Citations per Paper by Year')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Total citations per year\n",
    "    plt.subplot(2, 2, 2)\n",
    "    year_citations['sum'].plot(kind='bar', alpha=0.7)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Total Citations')\n",
    "    plt.title('Total Citations by Publication Year')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Citation vs age\n",
    "    plt.subplot(2, 2, 3)\n",
    "    current_year = enriched_df['year'].max()\n",
    "    enriched_df['age'] = current_year - enriched_df['year']\n",
    "    \n",
    "    age_citations = enriched_df.groupby('age')['cited_by_count'].mean()\n",
    "    age_citations.plot(kind='line', marker='o')\n",
    "    plt.xlabel('Paper Age (years)')\n",
    "    plt.ylabel('Mean Citations')\n",
    "    plt.title('Citations vs Paper Age')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot: year vs citations\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(enriched_df['year'], enriched_df['cited_by_count'], alpha=0.6, s=20)\n",
    "    plt.xlabel('Publication Year')\n",
    "    plt.ylabel('Citation Count')\n",
    "    plt.title('Citations vs Publication Year')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal and Venue Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze host venues from OpenAlex\n",
    "if 'host_venue_name' in enriched_df.columns:\n",
    "    venue_stats = enriched_df['host_venue_name'].value_counts().head(10)\n",
    "    \n",
    "    print(\"=== Top 10 Host Venues (OpenAlex) ===\")\n",
    "    display(venue_stats)\n",
    "    \n",
    "    # Compare with original journal field\n",
    "    if 'journal' in enriched_df.columns:\n",
    "        # Venue matching analysis\n",
    "        venue_matches = 0\n",
    "        total_with_both = 0\n",
    "        \n",
    "        for _, row in enriched_df.iterrows():\n",
    "            journal = str(row.get('journal', '')).lower()\n",
    "            venue = str(row.get('host_venue_name', '')).lower()\n",
    "            \n",
    "            if journal and venue and journal != 'nan' and venue != 'nan':\n",
    "                total_with_both += 1\n",
    "                if journal in venue or venue in journal:\n",
    "                    venue_matches += 1\n",
    "        \n",
    "        match_rate = venue_matches / total_with_both * 100 if total_with_both > 0 else 0\n",
    "        print(f\"\\nVenue matching: {venue_matches}/{total_with_both} ({match_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Access Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze open access status\n",
    "if 'is_oa' in enriched_df.columns:\n",
    "    oa_stats = enriched_df['is_oa'].value_counts()\n",
    "    oa_rate = enriched_df['is_oa'].mean() * 100 if enriched_df['is_oa'].notna().any() else 0\n",
    "    \n",
    "    print(f\"=== Open Access Statistics ===\")\n",
    "    print(f\"Open Access rate: {oa_rate:.1f}%\")\n",
    "    print(oa_stats)\n",
    "    \n",
    "    if 'oa_status' in enriched_df.columns:\n",
    "        oa_status_counts = enriched_df['oa_status'].value_counts()\n",
    "        print(f\"\\nOA Status breakdown:\")\n",
    "        display(oa_status_counts)\n",
    "        \n",
    "        # Visualize OA status\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        oa_stats.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Open Access Distribution')\n",
    "        plt.ylabel('')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        oa_status_counts.plot(kind='bar', alpha=0.7)\n",
    "        plt.xlabel('OA Status')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Open Access Status Types')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Summary and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cache summary\n",
    "final_cache_summary = save_cache_summary(str(cache_dir))\n",
    "\n",
    "print(\"=== Final Cache Summary ===\")\n",
    "for key, value in final_cache_summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Performance metrics\n",
    "success_rate = final_cache_summary['successful'] / final_cache_summary['total_files'] * 100 if final_cache_summary['total_files'] > 0 else 0\n",
    "print(f\"\\nAPI Success Rate: {success_rate:.1f}%\")\n",
    "\n",
    "# Estimate API calls made\n",
    "unique_dois = enriched_df['doi'].nunique()\n",
    "print(f\"Unique DOIs processed: {unique_dois}\")\n",
    "print(f\"Cache files created: {final_cache_summary['total_files']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Enriched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final save of enriched dataset\n",
    "enriched_df.to_csv(data_dir / 'papers_enriched_final.csv', index=False)\n",
    "print(f\"Saved final enriched dataset: {len(enriched_df)} papers\")\n",
    "\n",
    "# Save enrichment summary\n",
    "enrichment_summary = {\n",
    "    'total_papers': len(enriched_df),\n",
    "    'papers_with_dois': enriched_df['doi'].notna().sum(),\n",
    "    'papers_enriched': enriched_df['openalex_id'].notna().sum(),\n",
    "    'enrichment_rate': enriched_df['openalex_id'].notna().mean(),\n",
    "    'total_citations': enriched_df['cited_by_count'].sum() if 'cited_by_count' in enriched_df.columns else 0,\n",
    "    'total_references': len(all_references) if 'all_references' in locals() else 0,\n",
    "    'internal_citations': len(internal_refs) if 'internal_refs' in locals() else 0,\n",
    "    'api_performance': final_cache_summary\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(data_dir / 'enrichment_summary.json', 'w') as f:\n",
    "    json.dump(enrichment_summary, f, indent=2, default=str)\n",
    "print(\"Saved enrichment summary\")\n",
    "\n",
    "# Display final statistics\n",
    "print(f\"\\n=== Final Enrichment Results ===\")\n",
    "print(f\"Papers enriched: {enrichment_summary['papers_enriched']}/{enrichment_summary['total_papers']} ({enrichment_summary['enrichment_rate']*100:.1f}%)\")\n",
    "print(f\"Total citations collected: {enrichment_summary['total_citations']:,}\")\n",
    "print(f\"Total references collected: {enrichment_summary['total_references']:,}\")\n",
    "print(f\"Internal citation network edges: {enrichment_summary['internal_citations']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "1. **Connected to OpenAlex API** with robust caching and rate limiting\n",
    "2. **Enriched paper metadata** with citation counts, referenced works, and venue information\n",
    "3. **Analyzed citation patterns** across time, clusters, and journals\n",
    "4. **Built reference networks** identifying internal citations within the corpus\n",
    "5. **Examined open access status** and publication venue consistency\n",
    "6. **Optimized API usage** with intelligent caching to avoid redundant requests\n",
    "\n",
    "**Key Results:**\n",
    "- Enriched **{enrichment_summary['papers_enriched']} papers** with OpenAlex data\n",
    "- Collected **{enrichment_summary['total_citations']:,} total citations**\n",
    "- Identified **{enrichment_summary['internal_citations']:,} internal citation links**\n",
    "- Achieved **{enrichment_summary['enrichment_rate']*100:.1f}% enrichment rate**\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to `04_networks_and_backbones.ipynb` for citation network analysis\n",
    "- Use enriched data for RPYS analysis and main path detection\n",
    "- Generate comprehensive dialog cards with citation-based insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
